{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:  #2980b9\"> \n",
    "\n",
    "What does this notebook do?\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Takes a lot of Done jobs and similar amount of Failed Jobs</li>\n",
    "  <li>Gives out a df for HBLI measurements of these jobs (pivotted and scaled)</li>\n",
    "  <li>Gives out a df of  Jobs table entries for these jobs</li>\n",
    "    <li>Feature Generation: SELECT Columns from each of above table, and JOIN them</li>\n",
    "    <li>Note that timestep is = 1</li>\n",
    "    <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:  #2980b9\"> \n",
    "\n",
    "List of files written by this nb to disk:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>more</li>\n",
    "  <li>more</li>\n",
    "  <li>more</li>\n",
    "    <li>more</li>\n",
    "    <li>more</li>\n",
    "    <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[A] Get Failed Jobs:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Jobs table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>HBLI table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This notebook contains:\n",
    "#\n",
    "# Here we take 500K failed jobs and 375K successful jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Jobs table: get all 'failed' jobs and store to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "failedJobs = pd.read_pickle('../pickles/all_failed_from_jobs_table.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 HBLI table: get all 'failed' jobs and store to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataHBLI_failed = pd.read_pickle('../pickles/all_failed_from_hbli_table.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataHBLI_failed.shape, len(dataHBLI_failed.JobID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[B] Get Done Jobs:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Jobs table: Load Done jobs & Store small size dfs into files  ('listofdfs_collector*') - call these files F</li>\n",
    "    <li> J: Pick few (847267) jobs from all of Done jobs</li>\n",
    "    <li> H: Read each small file of dfs (F), select rows that belong to this subset of jobs (J)</li>\n",
    "    <li> Append these read and selected dfs (H) into a list (L)</li>\n",
    "    <li> Concat these dfs in list L into one data frame</li>\n",
    "  <li>HBLI table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Jobs table: get all 'done' jobs and store to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "successJobs = pd.read_pickle('../pickles/all_done_from_jobs_table.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "successJobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "successJobs_picked = successJobs.sample(847267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "listofdonejobs = successJobs_picked.JobID.tolist()\n",
    "len(listofdonejobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of each filename from above so we can go through collector in small batches\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "path = \"/data/home/a1singh/lstm/pickles/\"\n",
    "\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "onlyfiles = [x for x in onlyfiles if 'listofdfs_collector' in x]\n",
    "onlyfiles #these files contain done HBLI (each file contains a list of dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Go through each file above, get list of dfs\n",
    "# Iterate through the list and append append df to 'selectedDonedfs'\n",
    "\n",
    "selectedDonedfs = []\n",
    "print(len(onlyfiles))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for f in onlyfiles:\n",
    "    # read list of dfs, pack into one, filter the df, append to outer list\n",
    "    #\n",
    "    counter += 1\n",
    "    alist = pd.read_pickle(path+f)\n",
    "    df    = pd.concat(alist, ignore_index = True) #pack the list of dfs to one df\n",
    "    \n",
    "    # filter df\n",
    "    A = df[df['JobID'].isin(listofdonejobs)].copy() # filter list of jobs picked from all done jobs\n",
    "    \n",
    "    # free memory\n",
    "    del df\n",
    "    del alist\n",
    "    \n",
    "    # store it\n",
    "    selectedDonedfs.append(A)\n",
    "    \n",
    "    # print success\n",
    "    print(counter, ' Stored :',A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(listofjobids), len(listofdonejobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This is a major step, concatenating these large dataframes\n",
    "dataHBLI_success = pd.concat(selectedDonedfs, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del selectedDonedfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(set(dataHBLI_success.JobID.tolist())) #dataHBLI is now ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^####################################\n",
    "####################################^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^####################################\n",
    "####################################^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^####################################\n",
    "####################################^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^####################################\n",
    "####################################^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect() #Clean memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Total number of rows in HeartBeatLoggingInfo (ans - 906,320,307)\n",
    "# number of distinct JobIDs (ans - 10,137,037) in HeartBeatLoggingInfo\n",
    "# number of distinct JobIDs in Jobs (ans - 10,485,197)\n",
    "# number of distinct JobGroup in Jobs (ans - 5405)\n",
    "# number of distinct hearbeattime entries (ans - 1,616,7003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[C] Combine (Stack them together):\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>allJobs : failedJobs + successJobs</li>\n",
    "  <li>allHBLI : dataHBLI_failed + dataHBLI_success</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Jobs tables pieces = (failed, success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "failedJobs.shape, successJobs_picked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "failedJobs.shape, successJobs_picked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pieces = (failedJobs, successJobs_picked)\n",
    "allJobs = pd.concat(pieces, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allJobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allJobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del failedJobs\n",
    "del successJobs_picked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine HBLI tables pieces = (failed, success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pieces = (dataHBLI_failed, dataHBLI_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI = pd.concat(pieces, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del dataHBLI_failed\n",
    "del dataHBLI_success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process HBLI table: allHBLI (pivot, dropna, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[D] Pivot HBLI:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\"> \n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI.Value = pd.to_numeric(allHBLI['Value'], errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI.HeartBeatTime = pd.to_datetime(allHBLI['HeartBeatTime'], errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of unique jobIDs in 10M rows of dataHB (ans-185879)\n",
    "allHBLI.JobID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of unique jobIDs in 10M rows of dataHB (ans-185879)\n",
    "allHBLI.JobID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of unique timestampes in 10M rows of dataHB (ans-242222)\n",
    "allHBLI.HeartBeatTime.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of unique timestampes in 10M rows of dataHB (ans-242222)\n",
    "allHBLI.HeartBeatTime.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 10 millions rows took 10 sec to pivot\n",
    "\n",
    "allHBLI_wide=allHBLI.pivot_table(index=['JobID','HeartBeatTime'], columns='Name', values='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1  million rows have shape (142863,  7) after pivot\n",
    "# 10 million rows have shape (1428574, 7) after pivot\n",
    "\n",
    "allHBLI_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1  million rows have shape (142863,  7) after pivot\n",
    "# 10 million rows have shape (1428574, 7) after pivot\n",
    "\n",
    "allHBLI_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if allHBLI_wide.isnull().values.ravel().sum()>0:\n",
    "    # Drop the last few rows\n",
    "    allHBLI_wide = allHBLI_wide.dropna()\n",
    "    print('Dropped NAs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale 'allHBLI_wide' table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[E] Scale HBLI:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Use MinMaxScaler</li>\n",
    "  <li>Store scaler after fit</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "allHBLI_wide[['AvailableDiskSpace', 'CPUConsumed', 'LoadAverage', \n",
    "             'MemoryUsed', 'RSS','Vsize', 'WallClockTime']] = scaler.fit_transform(allHBLI_wide[['AvailableDiskSpace', \n",
    "                                                                        'CPUConsumed', 'LoadAverage', 'MemoryUsed', \n",
    "                                                                        'RSS','Vsize', 'WallClockTime']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('giga_minmaxscaler.pickle', 'wb') as handle:\n",
    "    pickle.dump(scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide = allHBLI_wide.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "allHBLI_wide.to_pickle('giga_allHBLI_wide_scaled.pickle')\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('large_allHBLI_wide_scaled_4742707by9.pickle', 'rb') as handle:\n",
    "    allHBLI_wide = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of HBLI table processing: allHBLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Jobs tables: allJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[F] Reduce width of Jobs table:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Prune columns of Jobs table:</li>\n",
    "  <li>Remove timestamp related fields</li>\n",
    "  <li>Remove fields with 1 states</li>\n",
    "  <li>Remove fields minorstatus and applicationstatus</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataJobs = allJobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataJobs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listofcolumns = dataJobs.columns.tolist()\n",
    "listofcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "removelist = ['JobID','JobName','SubmissionTime', 'RescheduleTime', 'LastUpdateTime', \n",
    "              'StartExecTime', 'HeartBeatTime', 'EndExecTime']\n",
    "\n",
    "for r in removelist:\n",
    "    listofcolumns.remove(r)\n",
    "    \n",
    "print(listofcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataJobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataJobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cc = {}\n",
    "\n",
    "for c in listofcolumns:\n",
    "    #print(c)\n",
    "    ll = len(dataJobs[c].unique().tolist())\n",
    "    \n",
    "    if ll > 0:\n",
    "        #print('************ found > limit *')\n",
    "        cc[c]=ll\n",
    "\n",
    "for w in sorted(cc, key=cc.get, reverse=False):\n",
    "  print(w, cc[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision: let us drop 'MinorStatus' and 'ApplicationStatus' and only predict \n",
    "### the 'Status' column\n",
    "#   LABEL for prediction: only predict the 'Status' column\n",
    "\n",
    "del dataJobs['MinorStatus']\n",
    "del dataJobs['ApplicationStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision: let us drop all columns which have only single state, i.e. no fluctuation\n",
    "\n",
    "for w in sorted(cc, key=cc.get, reverse=False):\n",
    "    if cc[w] == 1:\n",
    "        del dataJobs[w]\n",
    "        print(w, cc[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del dataJobs['JobName'] #name is not important for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataJobs.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataJobs[['SubmissionTime',\n",
    " 'RescheduleTime',\n",
    " 'LastUpdateTime',\n",
    " 'StartExecTime',\n",
    " 'HeartBeatTime',\n",
    " 'EndExecTime']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Decision: Convert times to deltas\n",
    "import numpy as np\n",
    "dataJobs['start_submit'] = (dataJobs['StartExecTime']-dataJobs['SubmissionTime']) / np.timedelta64(1, 'm')\n",
    "dataJobs['hbeat_start']  = (dataJobs['HeartBeatTime']-dataJobs['StartExecTime']) / np.timedelta64(1, 'm')\n",
    "\n",
    "for x in ['SubmissionTime','RescheduleTime','LastUpdateTime','StartExecTime','HeartBeatTime','EndExecTime']:\n",
    "    del dataJobs[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Decision: Remove the AccountedFlag at this stage\n",
    "del dataJobs['AccountedFlag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataJobs.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataJobs.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataJobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataJobs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[G] Jobs table: get_dummies:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Convert to categorical features</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Make categorical variables: (hold back on JobGroup dummies)\n",
    "dj_encoded = pd.get_dummies(dataJobs, columns=['JobType','Site','Status','UserPriority'],\n",
    "                           drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(dj_encoded.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dj_encoded.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Before dropping first column in get_dummies, this returned 4150\n",
    "len(dj_encoded[dj_encoded['Status_Failed']==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# After dropping first column in get_dummies, this returned 4150\n",
    "len(dj_encoded[dj_encoded['Status_Failed']==False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dj_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Important : this can be used later to make new partitions, with different features (eg jobgroup)\n",
    "######################################\n",
    "dj_encoded.to_pickle('giga_allJobs_dj_encoded.pickle')\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dj_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use this finally\n",
    "allJobs_encoded = dj_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allJobs_encoded.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del dataJobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del allHBLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### End of all jobs table processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tables for joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test Train split each one into 50%\n",
    "# Concat Test dfs and Train dfs,\n",
    "# Now you have 50% from each class in train and test.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide = pd.read_pickle('giga_allHBLI_wide_scaled.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allHBLI_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allHBLI_wide.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allJobs_encoded = pd.read_pickle('giga_allJobs_dj_encoded.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allJobs_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allJobs_encoded.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keep JobGroup \n",
    "\n",
    "lesscols = allJobs_encoded.columns.tolist()\n",
    "print(len(lesscols))\n",
    "\n",
    "### Extra removal after noticing high accuracy\n",
    "lesscols.remove('hbeat_start')\n",
    "lesscols.remove('start_submit')\n",
    "\n",
    "## Don't delete JobGroup here\n",
    "lesscols = [x for x in lesscols if x.find('Owner')==-1]\n",
    "\n",
    "##############################################\n",
    "\n",
    "len(lesscols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lesscols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allJobs_encodedsmall = allJobs_encoded[lesscols].copy()\n",
    "del allJobs_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allJobs_encoded = allJobs_encodedsmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allJobs_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[H] JOIN allHBLI_wide x allJobs_encoded:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>more</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#JOIN\n",
    "\n",
    "raw_samples = pd.merge(allHBLI_wide, allJobs_encoded, on =['JobID'])\n",
    "print(raw_samples.shape)\n",
    "\n",
    "#17M (17572524, 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# JOIN \n",
    "\n",
    "raw_samples = pd.merge(allHBLI_wide, allJobs_encoded, on =['JobID'])\n",
    "print(raw_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del allHBLI_wide\n",
    "del allJobs_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_samples.columns.tolist() #<<< get dummies from raw_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect(), raw_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dfff contains the JobGroup catef\n",
    "\n",
    "dfff = pd.get_dummies(raw_samples['JobGroup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del raw_samples['JobGroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del allJobs_encodedsmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect(), raw_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test code for breaking a df into batches\n",
    "fg = pd.DataFrame({'A':[x for x in range(10)]})\n",
    "for bat in batch([x for x in range(10)], int(10/5)):\n",
    "    print(type(fg.iloc[bat]))\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#3547F0\"> \n",
    "\n",
    "[@] Partition raw_samples into 21 parts and store each as file to disk\n",
    "\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Break raw_samples into batches (store this df as 21 files)\n",
    "# Store each batch into a file\n",
    "\n",
    "counter = 1\n",
    "for bat in batch([x for x in range(17576570)], int(17576570/20)):\n",
    "    print(counter, ':Started')\n",
    "    # Concat\n",
    "    raw_samples_k = pd.concat([raw_samples.iloc[bat], dfff.iloc[bat]], axis=1)\n",
    "    # Store\n",
    "    raw_samples_k.to_pickle('raw_samples_part_'+str(counter))\n",
    "    # Print\n",
    "    print(counter, ':Written to disk : ', raw_samples_k.shape)\n",
    "    # Clean\n",
    "    del raw_samples_k\n",
    "    gc.collect()\n",
    "    # Increment counters\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfff.shape, len(raw_samples.JobID.unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Pandas dataframe to LSTM 3D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thresholdd = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[I] Train-Test: Partition samples into cleaned (train) and takeoutdf (test) :\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>more</li>\n",
    "  <li>more</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2237C0\"> \n",
    "\n",
    "[ ] 20% :\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>more</li>\n",
    "  <li>more</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of unique JobIDs in cleaned\n",
    "totalleftjobslist = raw_samples.JobID.unique().tolist()\n",
    "totalleftjobs = len(raw_samples.JobID.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "indices = random.sample(range(len(totalleftjobslist)), int(totalleftjobs * .20))\n",
    "takeoutlist = [totalleftjobslist[i] for i in sorted(indices)]\n",
    "\n",
    "len(takeoutlist), totalleftjobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# List of JobIDs that will go to takeoutdf\n",
    "\n",
    "with open('giga_list_takeoutlist_for_testing.pickle', 'wb') as handle:\n",
    "    pickle.dump(takeoutlist, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del raw_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del raw_samples1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of jobs that go to cleaned 80% and takeoutdf 20% from raw_samples\n",
    "\n",
    "# Delete raw_samples\n",
    "\n",
    "# Loop: Read each stored file, fill up cleaned and takeoutdf one file at a time, delete\n",
    "\n",
    "# Stored both cleaned and takeoutdf to disk\n",
    "\n",
    "# Delete takeoutdf\n",
    "\n",
    "# Load cleaned, make X train etc, store them to disk, delete all from mem\n",
    "\n",
    "# Delete cleaned\n",
    "\n",
    "# Load takeoutdf, make X test etc, store them to disk, delete all from mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#3547F0\"> \n",
    "\n",
    "[@] Load raw_samples from 21 files: process 1 file as a time<br>\n",
    "[@] Create 'cleaned' and 'takeoutdf'\n",
    "\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get list of JobIDs that will go to takeoutdf (20%)\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "with open('giga_list_takeoutlist_for_testing.pickle', 'rb') as handle:\n",
    "    takeoutlist = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw_samples has been stored as a set of 21 files on disk\n",
    "# Get list of filenames in which raw_samples was broken and stored\n",
    "\n",
    "path = \".\"\n",
    "\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "onlyfiles = [x for x in onlyfiles if 'samples_part' in x]\n",
    "len(onlyfiles) #these files contain done HBLI (each file contains a list of dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# raw_samples has been stored as a set of 21 files on disk\n",
    "# Create 'cleaned' and 'takeoutdf' that contains JobGroup categorical columns\n",
    "# Reach each file of raw_samples, extract rows for cleaned and takeoutdf\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "cleaned_list   = []\n",
    "takeoutdf_list = []\n",
    "where = 1\n",
    "\n",
    "for f in onlyfiles:\n",
    "    print('Processing File : ', where, \" \", datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\"))\n",
    "    \n",
    "    gh = pd.read_pickle(f)\n",
    "    gh = gh.dropna() #dropna here - so cleaned and takeoutdf are ready to go\n",
    "    \n",
    "    cleaned_list.append(gh[~gh['JobID'].isin(takeoutlist)])\n",
    "    takeoutdf_list.append(gh[gh['JobID'].isin(takeoutlist)])\n",
    "    \n",
    "    del gh\n",
    "    \n",
    "    if(where == 1):\n",
    "        cleaned   = pd.concat(cleaned_list,   ignore_index = True)\n",
    "        takeoutdf = pd.concat(takeoutdf_list, ignore_index=True)\n",
    "    else:\n",
    "        cleaned_list.append(cleaned) #add existing df cleaned to list\n",
    "        cleaned   = pd.concat(cleaned_list,   ignore_index = True)\n",
    "        del cleaned_list\n",
    "        gc.collect()\n",
    "        \n",
    "        takeoutdf_list.append(takeoutdf) #add existing df takeoutdf to list\n",
    "        takeoutdf = pd.concat(takeoutdf_list, ignore_index=True)\n",
    "        del takeoutdf_list\n",
    "        gc.collect()\n",
    "    \n",
    "    print(cleaned.shape, takeoutdf.shape)\n",
    "\n",
    "    cleaned_list   = []\n",
    "    takeoutdf_list = []\n",
    "    where += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#3547F0\"> \n",
    "\n",
    "[@] Save takeoutdf as file to disk\n",
    "\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store takeoutdf\n",
    "with open('giga_takeoutdf_w_jobgroup.pickle', 'wb') as handle:\n",
    "    pickle.dump(takeoutdf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "takeoutdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del takeoutdf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#3547F0\"> \n",
    "\n",
    "[@] Partition cleaned into 5 parts and store each as file to disk\n",
    "\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Store cleaned to disk (in 5 parts)\n",
    "#\n",
    "# Break raw_samples into batches\n",
    "# Store each batch into a file\n",
    "\n",
    "counter = 1\n",
    "lim = cleaned.shape[0]\n",
    "\n",
    "for bat in batch([x for x in range(lim)], int(lim/4)):\n",
    "    print(counter, ':Started')\n",
    "          \n",
    "    # Operation: dump to disk\n",
    "    with open('giga_cleaned_w_jobgroup_part_'+str(counter)+'of_5.pickle', 'wb') as handle:\n",
    "        pickle.dump(cleaned.iloc[bat], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Print\n",
    "    print(counter, ':Written to disk : ', len(bat), lim)\n",
    "    # Clean\n",
    "    gc.collect()\n",
    "    # Increment counters\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on cleaned, Test on takeoutdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleaned.shape #(14058579, 3546) #(14058579, 3546))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build X (each job separately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[J1] Build Training features and labels from Partition 1 (cleaned):\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Jobs table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>HBLI table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gc.collect(), cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# time consuming step: No need anymore, since raw_samples took care of this (gh.dropna())\n",
    "\n",
    "# cleaned = cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleaned.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[J2] Build train_feat : list of training feature names\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Jobs table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>HBLI table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = cleaned.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dont remove Jobgroup\n",
    "\n",
    "train_feat = cleaned.columns.tolist()\n",
    "print(len(train_feat)) #125\n",
    "\n",
    "train_feat.remove('HeartBeatTime')\n",
    "train_feat.remove('WallClockTime')\n",
    "train_feat.remove('JobID')\n",
    "train_feat.remove('Status_Failed')\n",
    "\n",
    "##############################################\n",
    "\n",
    "len(train_feat) #122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feat.remove('RescheduleCounter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #f00\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[@] Break cleaned files into multiple X_train, Y_train small size files \n",
    "\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Label Column ('Status_Failed' is the label column)\n",
    "label_col = ['Status_Failed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholdd=1 #Check it matched the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Takes a df and converts it to 3D tensor\n",
    "# Each sample will have k time steps\n",
    "\n",
    "def samples_features(df_input):\n",
    "    \n",
    "    k = thresholdd\n",
    "    input_cols = train_feat\n",
    "    \n",
    "    # takes a df\n",
    "    # Put your inputs into a single list\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['single_input_vector'] = df_input[input_cols].apply(tuple, axis=1).apply(list)\n",
    "    \n",
    "    # Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements\n",
    "    df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])\n",
    "        \n",
    "    # The starting point\n",
    "    df['cumulative_input_vectors'] = df['single_input_vector'].shift(0)\n",
    "    \n",
    "    for i in range(1,k):\n",
    "        df['cumulative_input_vectors'] += df['single_input_vector'].shift(i)\n",
    "          \n",
    "    df.dropna(inplace=True)     # does operation in place & returns None\n",
    "\n",
    "    # Extract your training data\n",
    "    X_ = np.asarray(df.cumulative_input_vectors)\n",
    "    \n",
    "    # Use hstack to and reshape to make the inputs a 3d vector\n",
    "    X = np.vstack(X_).reshape(len(df), k, len(input_cols))\n",
    "    \n",
    "    # Clean up\n",
    "    del df\n",
    "    \n",
    "    return X\n",
    "    # returns 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Takes a df and converts it to 3D tensor\n",
    "# Each sample will have k time steps\n",
    "\n",
    "def samples_label(df_input):\n",
    "    \n",
    "    k = thresholdd\n",
    "    input_cols = label_col\n",
    "    \n",
    "    # takes a df\n",
    "    # Put your inputs into a single list\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['single_input_vector'] = df_input[input_cols].apply(tuple, axis=1).apply(list)\n",
    "    \n",
    "    # Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements\n",
    "    df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])\n",
    "        \n",
    "    # The starting point\n",
    "    df['cumulative_input_vectors'] = df['single_input_vector'].shift(0)\n",
    "    \n",
    "    for i in range(1,k):\n",
    "        df['cumulative_input_vectors'] += df['single_input_vector'].shift(i)\n",
    "          \n",
    "    df.dropna(inplace=True)     # does operation in place & returns None\n",
    "\n",
    "    # Extract your training data\n",
    "    X_ = np.asarray(df.cumulative_input_vectors)\n",
    "    \n",
    "    # Use hstack to and reshape to make the inputs a 3d vector\n",
    "    X = np.vstack(X_).reshape(len(df), k, len(input_cols))\n",
    "    \n",
    "    # Clean up\n",
    "    del df\n",
    "    \n",
    "    return X\n",
    "    # returns 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('giga_training_features_w_jobgroup.pickle', 'rb') as handle:\n",
    "    train_feat = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Unison shuffle\n",
    "def unison_shuffled_copies(a, b):\n",
    "    import numpy as np\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['giga_cleaned_w_jobgroup_part_1of_5.pickle',\n",
       " 'giga_cleaned_w_jobgroup_part_2of_5.pickle',\n",
       " 'giga_cleaned_w_jobgroup_part_3of_5.pickle',\n",
       " 'giga_cleaned_w_jobgroup_part_4of_5.pickle',\n",
       " 'giga_cleaned_w_jobgroup_part_5of_5.pickle']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of each filename from above so we can go through collector in small batches\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "path = \".\"\n",
    "\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "onlyfiles = [x for x in onlyfiles if 'giga_cleaned_w_jobgroup_part' in x]\n",
    "onlyfiles.sort() #these files contain done HBLI (each file contains a list of dataframes)\n",
    "setofcleanedfiles=onlyfiles\n",
    "setofcleanedfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def applyParallel(dfGrouped, func):\n",
    "    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n",
    "    return retLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breakeachinto = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genereate_training_set():\n",
    "    counter = 1\n",
    "    minicounter = 1\n",
    "    \n",
    "    # read each cleaned file\n",
    "    for f in setofcleanedfiles:\n",
    "        print(\"File : \", minicounter)\n",
    "        onedf = pd.read_pickle(f)\n",
    "        length = onedf.shape[0]\n",
    "        \n",
    "        for bat in batch([x for x in range(length)], int(length/breakeachinto)):\n",
    "            print(\"Batch : \", counter)\n",
    "            cleaned_sub    = onedf.iloc[bat]\n",
    "            cleanedgrouped = cleaned_sub.groupby('JobID')\n",
    "            print('Collectrain started...')\n",
    "            #\n",
    "            collecttrain   = applyParallel(cleanedgrouped, samples_features)\n",
    "            del cleaned_sub\n",
    "            X = []\n",
    "            for x in collecttrain:\n",
    "                #len(x)\n",
    "                for i in x:\n",
    "                    X.append(i)\n",
    "            del collecttrain\n",
    "            X = np.array(X)\n",
    "            #\n",
    "            print('Collectlabel started...')\n",
    "            collectlabel = applyParallel(cleanedgrouped, samples_label)\n",
    "            del cleanedgrouped\n",
    "            \n",
    "            Y=[]\n",
    "            for x in collectlabel:\n",
    "                #len(x)\n",
    "                for i in x:\n",
    "                    Y.append(i)\n",
    "            del collectlabel\n",
    "            Y  = np.array(Y)\n",
    "            YY = np.array([x[0][0] for x in Y]).reshape(len(Y),1)\n",
    "            # \n",
    "            X_final, Y_final = unison_shuffled_copies(X,YY)\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "            # picle X_final, Y_final\n",
    "            print('Writing X_final to disk')\n",
    "            with open('X_final_giga_long_'+'_file_'+str(minicounter)+'_'+str(counter)+'.pickle', 'wb') as handle:\n",
    "                pickle.dump(X_final, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            del X_final\n",
    "            gc.collect()\n",
    "            \n",
    "            print('Writing Y_final to disk')\n",
    "            with open('Y_final_giga_long_'+'_file_'+str(minicounter)+'_'+str(counter)+'.pickle', 'wb') as handle:\n",
    "                pickle.dump(Y_final, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "            del y_final\n",
    "            gc.collect()\n",
    "            counter +=1\n",
    "        # end of inner for loop\n",
    "        del onedf\n",
    "    #outer for that goes over each file\n",
    "    minicounter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genereate_training_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #f00\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[@] Break takeoutdf into multiple X_test, Y_test small size files\n",
    "\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['giga_takeoutdf_w_jobgroup.pickle']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of each filename from above so we can go through collector in small batches\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "path = \".\"\n",
    "\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "onlyfiles = [x for x in onlyfiles if 'giga_takeoutdf_w_jobgroup' in x]\n",
    "onlyfiles.sort() #these files contain done HBLI (each file contains a list of dataframes)\n",
    "setoftakeoutdffiles=onlyfiles\n",
    "setoftakeoutdffiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genereate_test_set():\n",
    "    counter = 1\n",
    "    minicounter = 1\n",
    "    \n",
    "    # read each cleaned file\n",
    "    for f in setoftakeoutdffiles:\n",
    "        print(\"File : \", minicounter)\n",
    "        onedf = pd.read_pickle(f)\n",
    "        length = onedf.shape[0]\n",
    "        \n",
    "        # variable name *cleaned has not been changed here:\n",
    "        for bat in batch([x for x in range(length)], int(length/breakeachinto)):\n",
    "            print(\"Batch : \", counter)\n",
    "            cleaned_sub    = onedf.iloc[bat]\n",
    "            cleanedgrouped = cleaned_sub.groupby('JobID')\n",
    "            print('Collectrain started...')\n",
    "            collecttrain   = applyParallel(cleanedgrouped, samples_features)\n",
    "            del cleaned_sub\n",
    "            X = []\n",
    "            for x in collecttrain:\n",
    "                #len(x)\n",
    "                for i in x:\n",
    "                    X.append(i)\n",
    "            del collecttrain\n",
    "            X = np.array(X)\n",
    "            #\n",
    "            # pickle X\n",
    "            print('Collectlabel started...')\n",
    "            collectlabel = applyParallel(cleanedgrouped, samples_label)\n",
    "            del cleanedgrouped\n",
    "            \n",
    "            Y=[]\n",
    "            for x in collectlabel:\n",
    "                #len(x)\n",
    "                for i in x:\n",
    "                    Y.append(i)\n",
    "            del collectlabel\n",
    "            Y  = np.array(Y)\n",
    "            YY = np.array([x[0][0] for x in Y]).reshape(len(Y),1)\n",
    "            # \n",
    "            X_test, Y_test = X,YY # no shuffling in test set\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "            # picle X_final, Y_final\n",
    "            print('Writing X_test to disk')\n",
    "            with open('X_test_giga_long_'+'_file_'+str(minicounter)+'_'+str(counter)+'.pickle', 'wb') as handle:\n",
    "                pickle.dump(X_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            del X_test\n",
    "            gc.collect()\n",
    "            \n",
    "            print('Writing Y_test to disk')\n",
    "            with open('Y_test_giga_long_'+'_file_'+str(minicounter)+'_'+str(counter)+'.pickle', 'wb') as handle:\n",
    "                pickle.dump(Y_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "            del y_test\n",
    "            \n",
    "            gc.collect()\n",
    "            counter +=1\n",
    "        # end of inner for loop\n",
    "        del onedf\n",
    "    #outer for that goes over each file\n",
    "    minicounter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File :  1\n",
      "Batch :  1\n",
      "Collectrain started...\n",
      "Collect label started...\n",
      "Writing X_test to disk\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6758b0738f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenereate_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-4764dcaed1ec>\u001b[0m in \u001b[0;36mgenereate_test_set\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Writing X_test to disk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_giga_long_test'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_file_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminicounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "genereate_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ End of breaking cleaned files into smalled X_train, Y_train files\n",
    "############ End of breaking takeoutdf files into smalled X_test, Y_test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #f00\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleanedgrouped = cleaned.groupby('JobID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del cleaned\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(thresholdd), cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('giga_training_features_w_jobgroup.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_feat, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned df to 3D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "collecttrain = applyParallel(cleanedgrouped, samples_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleanedgrouped is needed later for test labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(collecttrain) # old: 801846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('giga_collecttrain.pickle', 'wb') as handle:\n",
    "    pickle.dump(collecttrain, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# >> Start here for importing training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack X from output of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=[]\n",
    "for x in collecttrain:\n",
    "    #len(x)\n",
    "    for i in x:\n",
    "        X.append(i)\n",
    "X = np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del collecttrain\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## X is ready (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Label Column ('Status_Failed' is the label column)\n",
    "label_col = ['Status_Failed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Y (each job separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "collectlabel = applyParallel(cleanedgrouped, samples_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del cleanedgrouped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('giga_collectlabel.pickle', 'wb') as handle:\n",
    "    pickle.dump(collectlabel, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(collectlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Unpack Y from output of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y=[]\n",
    "for x in collectlabel:\n",
    "    #len(x)\n",
    "    for i in x:\n",
    "        Y.append(i)\n",
    "Y  = np.array(Y)\n",
    "YY = np.array([x[0][0] for x in Y]).reshape(len(Y),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del collectlabel\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "YY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle X,Y in Unison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_final, Y_final = unison_shuffled_copies(X,YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_final, Y_final = unison_shuffled_copies(X_final, Y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_final.shape, Y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save X_final, Y_final to disk\n",
    "import pickle\n",
    "\n",
    "with open('X_final8020_w_JobGroup.pickle.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_final, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_final\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Y_final8020_w_JobGroup.pickle', 'wb') as handle:\n",
    "    pickle.dump(Y_final, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Y_final\n",
    "del X\n",
    "del Y\n",
    "del YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start: Build test set from 'takeoutdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build X_test, Y_test using each JobID separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[K] Build Testing features and labels from Partition 2 (takeoutdf):\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Extract X_test, then unpack it</li>\n",
    "  <li>Extract Y_test, then unpack it</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "takeoutdf = pd.read_pickle('giga_takeoutdf_w_jobgroup.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(train_feat)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Label Column ('Status_Failed' is the label column)\n",
    "print(label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('giga_training_features_w_jobgroup.pickle', 'rb') as handle:\n",
    "    train_feat = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "takeoutdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# takeoutdf = takeoutdf.dropna() # not needed since raw_samples takes care of this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "takeoutdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extract X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "takeoutdfgrouped = takeoutdf.groupby('JobID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del takeoutdf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "collectXtest = applyParallel(takeoutdfgrouped, samples_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('giga_collectXtest.pickle', 'wb') as handle:\n",
    "    pickle.dump(collectXtest, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testcase 1 of function sample_features:\n",
    "\n",
    "check = 0\n",
    "j_checks = takeoutdf.JobID.sample(20)\n",
    "\n",
    "for k in j_checks:\n",
    "    \n",
    "    v = takeoutdfgrouped.get_group(k)\n",
    "    \n",
    "    if(True):\n",
    "        #print(v.columns.tolist())\n",
    "        A = np.array(v)\n",
    "        #print(train_feat)\n",
    "        B = samples_features(v)\n",
    "        check += 1\n",
    "        \n",
    "        if(len(A) == len(B)):\n",
    "            print(k, 'Lengths match')\n",
    "        else:\n",
    "            print(k, 'Lengths NOT match')\n",
    "        \n",
    "        # Tests\n",
    "        for x,y in zip(A,B):\n",
    "            #print(x)\n",
    "            #print(y)\n",
    "            \n",
    "            if not (np.all([ m==n for (m,n) in zip(x[2:8] , y[0][0:6]) ])):\n",
    "                print(k, 'NOK')\n",
    "                \n",
    "    if(check == len(j_checks)):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "takeoutdf.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "collectXtest[100].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for x in collectXtest:\n",
    "    #len(x)\n",
    "    for i in x:\n",
    "        X_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del collectXtest\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## X_test is ready\n",
    "# Save\n",
    "\n",
    "with open('X_test8020_w_jobgroup.pickle', 'wb') as handle:\n",
    "    pickle.dump(X_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# Delete\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "collecttestlabel = applyParallel(takeoutdfgrouped, samples_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del takeoutdfgrouped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(collecttestlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mega_collecttestlabel.pickle', 'wb') as handle:\n",
    "    pickle.dump(collecttestlabel, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_t=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for x in collecttestlabel:\n",
    "    #len(x)\n",
    "    for i in x:\n",
    "        Y_t.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del collecttestlabel\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_t = np.array(Y_t)\n",
    "Y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_test = np.array([x[0][0] for x in Y_t]).reshape(len(Y_t),1)\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del Y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## X_test is ready\n",
    "# Save\n",
    "\n",
    "with open('Y_test8020_w_jobgroup.pickle', 'wb') as handle:\n",
    "    pickle.dump(Y_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# Delete\n",
    "del Y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Y_test is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End test set from 'takeoutdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[L] Build Model 1, Train, Evaluate, Save, Get Stats, Confusion Matrix:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Model 1: [20,20]</li>\n",
    "  <li>Model 2: [50,50,50,50]</li>\n",
    "  <li>Model 3: [100,100,100,100,100,100]</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_length = X_final.shape[1]\n",
    "\n",
    "input_dim = X_final.shape[2]\n",
    "\n",
    "# Output dimensions is the shape of a single output vector\n",
    "# In this case it's just 1, but it could be more\n",
    "output_dim = len(Y_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum([x[0] for x in Y_final])/ len(Y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_final[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "input_dim, input_length, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model 1\n",
    "\n",
    "def create_model1(input_dim = input_dim, input_length = input_length, output_dim=output_dim):\n",
    "    print ('Creating model 1...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(20, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(20))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "    print ('Compiling...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model 2\n",
    "\n",
    "def create_model2(input_dim = input_dim, input_length = input_length, output_dim=output_dim):\n",
    "    print ('Creating model 2...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(50, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(50, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "    print ('Compiling...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model 3\n",
    "\n",
    "def create_model3(input_dim = input_dim, input_length = input_length, output_dim=output_dim):\n",
    "    print ('Creating model 3...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(100, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(100, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(100, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(100, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "    print ('Compiling...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model1 = create_model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print ('Fitting model...')\n",
    "history = model1.fit(X_final,Y_final,batch_size=250, epochs=100, validation_split = 0.10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model1.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# when taking 10 timesteps, and keeping separate test and training JobIDs\n",
    "# loss, accuracy is: \n",
    "# on X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model1.save('mega_model1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred[15000:15010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model1.save('mega_model1.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_true = pd.Series([x[0] for x in Y_test])\n",
    "\n",
    "# Use np.rint for rounding off \n",
    "y_predicted = pd.Series([ np.rint(j[0]) for j in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(np.equal(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(y_true, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_true, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Worst case error bound on JobID specific 'Status' prediction error:\n",
    "\n",
    "100*(9155+48481)/2368345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(takeoutdf.JobID.unique().tolist()), len(cleaned.JobID.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum(takeoutdf[\"JobID\"].value_counts()==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(X_test[(y_true==1) & (y_predicted==0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verify that there is no intersection between training and test jobs\n",
    "\n",
    "list(set(takeoutdf.JobID.tolist()) & set(cleaned.JobID.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predicting all 0s ??\n",
    "sum([ (j) for j in y_predicted]), len(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('mega_training_features.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_feat, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Model 2 Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get min/max timestamp on cleaned & takeoutdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[M] Build Model 2, Train, Evaluate, Save, Get Stats, Confusion Matrix:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Jobs table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>HBLI table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2 = create_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print ('Fitting model...')\n",
    "history = model2.fit(X_final,Y_final,batch_size=250, epochs=100, validation_split = 0.10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model2.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# when taking 10 timesteps, and keeping separate test and training JobIDs\n",
    "# loss, accuracy is: \n",
    "# on X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.save('mega_model2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred[15000:15010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_true = pd.Series([x[0] for x in Y_test])\n",
    "\n",
    "# Use np.rint for rounding off \n",
    "y_predicted = pd.Series([ np.rint(j[0]) for j in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(np.equal(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(y_true, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_true, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Worst case error bound on JobID specific 'Status' prediction error:\n",
    "\n",
    "100*(7618+46967)/2368345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(takeoutdf.JobID.unique().tolist()), len(cleaned.JobID.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum(takeoutdf[\"JobID\"].value_counts()==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(X_test[(y_true==1) & (y_predicted==0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verify that there is no intersection between training and test jobs\n",
    "\n",
    "list(set(takeoutdf.JobID.tolist()) & set(cleaned.JobID.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predicting all 0s ??\n",
    "sum([ (j) for j in y_predicted]), len(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Model 2 End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Model 3 Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2467C0\"> \n",
    "\n",
    "[N] Build Model 3, Train, Evaluate, Save, Get Stats, Confusion Matrix:\n",
    "\n",
    "</h2>\n",
    "\n",
    "<ul style=\"list-style-type:square\">\n",
    "  <li>Jobs table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>HBLI table: Load failed jobs from file to df and store to disk as df</li>\n",
    "  <li>more</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model3 = create_model3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print ('Fitting model...')\n",
    "history = model3.fit(X_final,Y_final,batch_size=250, epochs=100, validation_split = 0.10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model3.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# when taking 10 timesteps, and keeping separate test and training JobIDs\n",
    "# loss, accuracy is: \n",
    "# on X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred[15000:15010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model3.save('large_model3_tsteps1_features65_partII.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_true = pd.Series([x[0] for x in Y_test])\n",
    "\n",
    "# Use np.rint for rounding off \n",
    "y_predicted = pd.Series([ np.rint(j[0]) for j in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(np.equal(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(y_true, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_true, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Worst case error bound on JobID specific 'Status' prediction error:\n",
    "\n",
    "100*(7440+45791)/2368345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(takeoutdf.JobID.unique().tolist()), len(cleaned.JobID.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum(takeoutdf[\"JobID\"].value_counts()==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(X_test[(y_true==1) & (y_predicted==0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verify that there is no intersection between training and test jobs\n",
    "\n",
    "len(set(takeoutdf.JobID.tolist()) & set(cleaned.JobID.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(set(takeoutdf.JobID.tolist())), len(set(cleaned.JobID.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predicting all 0s ??\n",
    "sum([ (j) for j in y_predicted]), len(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Model 3 End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid code above this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
