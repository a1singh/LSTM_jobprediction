{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8778415, 88)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "takeoutdf = pd.read_pickle('mega_takeoutdf_for_testing.pickle')\n",
    "takeoutdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8794109, 88)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = pd.read_pickle('mega_cleaned_for_training.pickle')\n",
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2015-08-07 13:29:48'), Timestamp('2015-08-07 13:13:48'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "takeoutdf.HeartBeatTime.min(), cleaned.HeartBeatTime.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, Timestamp('2017-04-01 00:22:43'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "takeoutdf.HeartBeatTime.max().month, cleaned.HeartBeatTime.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,1):\n",
    "    print('sdfasdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def checkpoint(alist, anum):\n",
    "    # put the list of dfs to disk\n",
    "    filename = 'testerdeletethis'+str(anum)+'percent.pickle'\n",
    "    print('Checkpoint func > writing file: ', filename)\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(alist, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint func > writing file:  testerdeletethis1percent.pickle\n"
     ]
    }
   ],
   "source": [
    "checkpoint( [23,23,23,23,23], 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testerdeletethis1percent.pickle', 'rb') as handle:\n",
    "        lt = pickle.load( handle )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 23, 23, 23, 23]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymysql.connections.Connection object at 0x7f19597a59b0>\n"
     ]
    }
   ],
   "source": [
    "# Connect to MYSQL database\n",
    "\n",
    "hostname = 'localhost'\n",
    "username = 'a1singh'\n",
    "password = 'sdsc1234'\n",
    "database = 'belle2'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymysql\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "conn = pymysql.connect(host=hostname, user=username, passwd=password, db=database)\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()\n",
    "\n",
    "def applyParallel(dfGrouped, func):\n",
    "    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n",
    "    return retLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('large_cleaned_for_training.pickle', 'rb') as handle:\n",
    "    cleaned = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedjobids = list(set(cleaned.JobID.tolist()))\n",
    "\n",
    "# Takes a df and converts it to 3D tensor\n",
    "# Each sample will have k time steps\n",
    "\n",
    "def samples_features_get_first(df_input):\n",
    "    \n",
    "    k = thresholdd\n",
    "    input_cols = train_feat\n",
    "    \n",
    "    # takes a df\n",
    "    # Put your inputs into a single list\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['single_input_vector'] = df_input[input_cols].head(1).apply(tuple, axis=1).apply(list)\n",
    "    \n",
    "    # Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements\n",
    "    df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])\n",
    "        \n",
    "    # The starting point\n",
    "    df['cumulative_input_vectors'] = df['single_input_vector'].shift(0)\n",
    "    \n",
    "    for i in range(1,k):\n",
    "        df['cumulative_input_vectors'] += df['single_input_vector'].shift(i)\n",
    "          \n",
    "    df.dropna(inplace=True)     # does operation in place & returns None\n",
    "\n",
    "    # Extract your training data\n",
    "    X_ = np.asarray(df.cumulative_input_vectors)\n",
    "    \n",
    "    # Use hstack to and reshape to make the inputs a 3d vector\n",
    "    X = np.vstack(X_).reshape(len(df), k, len(input_cols))\n",
    "    \n",
    "    # Clean up\n",
    "    del df\n",
    "    \n",
    "    return X\n",
    "    # returns 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n"
     ]
    }
   ],
   "source": [
    "cols = cleaned.columns.tolist()\n",
    "train_feat = cleaned.columns.tolist()\n",
    "print(len(train_feat)) #125\n",
    "\n",
    "train_feat.remove('HeartBeatTime')\n",
    "train_feat.remove('JobID')\n",
    "train_feat.remove('Status_Failed')\n",
    "\n",
    "\n",
    "### Extra removal after noticing high accuracy\n",
    "train_feat.remove('hbeat_start')\n",
    "train_feat.remove('start_submit')\n",
    "train_feat.remove('WallClockTime')\n",
    "\n",
    "for c in cols:\n",
    "    if 'JobGroup' in c:\n",
    "        train_feat.remove(c)\n",
    "##############################################\n",
    "\n",
    "train_feat = ['AvailableDiskSpace',\n",
    " 'CPUConsumed',\n",
    " 'LoadAverage',\n",
    " 'MemoryUsed',\n",
    " 'RSS',\n",
    " 'Vsize',\n",
    " 'RescheduleCounter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 4)\n",
      "(133, 4)\n",
      "(217, 4)\n",
      "(287, 4)\n",
      "(217, 4)\n",
      "(140, 4)\n",
      "(189, 4)\n",
      "(161, 4)\n",
      "(182, 4)\n",
      "(126, 4)\n",
      "(161, 4)\n",
      "(315, 4)\n",
      "(203, 4)\n",
      "(357, 4)\n",
      "(196, 4)\n",
      "(112, 4)\n",
      "(112, 4)\n",
      "(203, 4)\n",
      "(210, 4)\n",
      "(28, 4)\n",
      "(21, 4)\n",
      "(126, 4)\n",
      "(35, 4)\n",
      "(28, 4)\n",
      "(84, 4)\n",
      "(287, 4)\n",
      "(161, 4)\n",
      "(70, 4)\n",
      "(273, 4)\n",
      "(105, 4)\n",
      "(63, 4)\n",
      "(49, 4)\n",
      "(140, 4)\n",
      "(28, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "thresholdd = 1\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "        \n",
    "# alljobsids\n",
    "# remainingjobids = alljobids - cleaned - takeoutdf\n",
    "\n",
    "for listofjobids in batch(cleanedjobids[:100], 3):\n",
    "    # Get HBLI\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Process HBLI\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get Jobs\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Process Jobs\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Join\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get X_batch, Y_batch\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Perform Model.evaluate on this batch\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Print accuracy and loss for this batch\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Store results in two lists: Y_big_true, Y_big_predicted\n",
    "    \n",
    "    \n",
    "    query       = 'SELECT * FROM HeartBeatLoggingInfo WHERE JobID IN (' + ','.join((str(x) for x in listofjobids)) + ')'\n",
    "    hblichunck  = pd.read_sql_query(query,con=conn)\n",
    "    #print(hblichunck)\n",
    "    \n",
    "    # Pivot\n",
    "    #hblichunck=hblichunck.pivot_table(index=['JobID','HeartBeatTime'], columns='Name', values='Value')\n",
    "    #hblichunckgrouped = hblichunck.groupby('JobID')\n",
    "\n",
    "    #collect_1st_extreme = applyParallel(hblichunckgrouped, samples_features_get_first)\n",
    "    print(hblichunck.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # Keep only the rows whose jobID is in HeartBeatLogging table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
