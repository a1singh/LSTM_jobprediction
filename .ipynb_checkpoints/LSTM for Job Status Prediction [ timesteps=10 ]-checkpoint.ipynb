{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook contains:\n",
    "#\n",
    "# Functions to connect to MYSQL database and generate list of feature vectors for each JOBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MYSQL database\n",
    "\n",
    "hostname = 'localhost'\n",
    "username = 'a1singh'\n",
    "password = 'sdsc1234'\n",
    "database = 'belle2'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "conn = pymysql.connect(host=hostname, user=username, passwd=password, db=database)\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"show tables\"\n",
    "df = pd.read_sql(query, conn)\n",
    "tables = df['Tables_in_belle2'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tname in tables:\n",
    "    print(tname)\n",
    "    df = pd.read_sql(\"SHOW COLUMNS FROM \"+tname, conn)\n",
    "    display(df)\n",
    "    x = df.as_matrix()\n",
    "    print(x)\n",
    "    print(len(x))\n",
    "    print(100*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Total number of rows in HeartBeatLoggingInfo (ans - 906,320,307)\n",
    "\n",
    "df = pd.read_sql(\"SELECT COUNT(*) FROM HeartBeatLoggingInfo\", conn)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Total number of unique JobIDs in HeartBeatLoggingInfo\n",
    "# number of distinct JobIDs (ans - 10,137,037)\n",
    "\n",
    "df = pd.read_sql(\"SELECT COUNT(DISTINCT JobID) FROM HeartBeatLoggingInfo LIMIT 10\", conn)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Total number of unique timestamps in HeartBeatLoggingInfo\n",
    "# number of distinct hearbeattime entries (ans - 1,616,7003)\n",
    "\n",
    "df_stamps = pd.read_sql(\"SELECT COUNT(DISTINCT HeartBeatTime) FROM HeartBeatLoggingInfo LIMIT 10\", conn)\n",
    "display(df_stamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Number of timestamps per jobID\n",
    "\n",
    "16167003 / 10137037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of key-value pairs per timestamp\n",
    "\n",
    "906320307 / 16167003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Failed Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "query       = 'SELECT * FROM Jobs WHERE Status = \\'Failed\\' LIMIT 133000'\n",
    "failedJobs    = pd.read_sql_query(query,con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failedJobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofjobids = failedJobs['JobID'].tolist()\n",
    "listofjobids = list(set(listofjobids))\n",
    "len(listofjobids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # Keep only the rows whose jobID is in HeartBeatLogging table\n",
    "\n",
    "query       = 'SELECT * FROM HeartBeatLoggingInfo WHERE JobID IN (' + ','.join((str(x) for x in listofjobids)) + ')'\n",
    "dataHBLI_failed    = pd.read_sql_query(query,con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataHBLI_failed.JobID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Successful Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find equal number of Successful job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "query       = 'SELECT * FROM Jobs WHERE Status = \\'Done\\' LIMIT 100000'\n",
    "successJobs    = pd.read_sql_query(query,con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successJobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofjobids = successJobs['JobID'].tolist()\n",
    "listofjobids = list(set(listofjobids))\n",
    "len(listofjobids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # Keep only the rows whose jobID is in HeartBeatLogging table\n",
    "\n",
    "query   = 'SELECT * FROM HeartBeatLoggingInfo WHERE JobID IN (' + ','.join((str(x) for x in listofjobids)) + ')'\n",
    "dataHBLI_success = pd.read_sql_query(query,con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataHBLI_success.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataHBLI_success.JobID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine HBLI tables pieces = (failed, success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataHBLI_failed.shape, dataHBLI_success.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = (dataHBLI_failed, dataHBLI_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI = pd.concat(pieces, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Jobs tables pieces = (failed, success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failedJobs.shape, successJobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = (failedJobs, successJobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allJobs = pd.concat(pieces, ignore_index = True)\n",
    "allJobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process HBLI table: allHBLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI.Value = pd.to_numeric(allHBLI['Value'], errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI.HeartBeatTime = pd.to_datetime(allHBLI['HeartBeatTime'], errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique jobIDs in 10M rows of dataHB (ans-185879)\n",
    "allHBLI.JobID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique timestampes in 10M rows of dataHB (ans-242222)\n",
    "allHBLI.HeartBeatTime.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 10 millions rows took 10 sec to pivot\n",
    "\n",
    "allHBLI_wide=allHBLI.pivot_table(index=['JobID','HeartBeatTime'], columns='Name', values='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1  million rows have shape (142863,  7) after pivot\n",
    "# 10 million rows have shape (1428574, 7) after pivot\n",
    "\n",
    "allHBLI_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI_wide.isnull().values.ravel().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the last few rows\n",
    "allHBLI_wide = allHBLI_wide.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI_wide.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI_wide.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "allHBLI_wide[['AvailableDiskSpace', 'CPUConsumed', 'LoadAverage', \n",
    "             'MemoryUsed', 'RSS','Vsize', 'WallClockTime']] = scaler.fit_transform(allHBLI_wide[['AvailableDiskSpace', \n",
    "                                                                        'CPUConsumed', 'LoadAverage', 'MemoryUsed', \n",
    "                                                                        'RSS','Vsize', 'WallClockTime']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI_wide.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI_wide = allHBLI_wide.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "allHBLI_wide.to_pickle('allHBLI_wide_scaled.pickle')\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of HBLI table processing: allHBLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Jobs tables: allJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join allJobs tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJobs = allJobs.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofcolumns = dataJobs.columns.tolist()\n",
    "#listofcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removelist = ['JobID','JobName','SubmissionTime', 'RescheduleTime', 'LastUpdateTime', \n",
    "              'StartExecTime', 'HeartBeatTime', 'EndExecTime']\n",
    "\n",
    "for r in removelist:\n",
    "    listofcolumns.remove(r)\n",
    "    \n",
    "print(listofcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = {}\n",
    "\n",
    "for c in listofcolumns:\n",
    "    #print(c)\n",
    "    ll = len(dataJobs[c].unique().tolist())\n",
    "    \n",
    "    if ll > 0:\n",
    "        #print('************ found > limit *')\n",
    "        cc[c]=ll\n",
    "\n",
    "for w in sorted(cc, key=cc.get, reverse=False):\n",
    "  print(w, cc[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision: let us drop 'MinorStatus' and 'ApplicationStatus' and only predict the 'Status' column\n",
    "#   LABEL for prediction: only predict the 'Status' column\n",
    "\n",
    "del dataJobs['MinorStatus']\n",
    "del dataJobs['ApplicationStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision: let us drop all columns which have only single state, i.e. no fluctuation\n",
    "for w in sorted(cc, key=cc.get, reverse=False):\n",
    "    if cc[w] == 1:\n",
    "        del dataJobs[w]\n",
    "        print(w, cc[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataJobs['JobName'] #name is not important for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJobs.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJobs[['SubmissionTime',\n",
    " 'RescheduleTime',\n",
    " 'LastUpdateTime',\n",
    " 'StartExecTime',\n",
    " 'HeartBeatTime',\n",
    " 'EndExecTime']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision: Convert times to deltas\n",
    "\n",
    "dataJobs['start_submit'] = (dataJobs['StartExecTime']-dataJobs['SubmissionTime']) / np.timedelta64(1, 'm')\n",
    "dataJobs['hbeat_start']  = (dataJobs['HeartBeatTime']-dataJobs['StartExecTime']) / np.timedelta64(1, 'm')\n",
    "\n",
    "for x in ['SubmissionTime','RescheduleTime','LastUpdateTime','StartExecTime','HeartBeatTime','EndExecTime']:\n",
    "    del dataJobs[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision: Remove the AccountedFlag at this stage\n",
    "del dataJobs['AccountedFlag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJobs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJobs.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataJobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dj_encoded = pd.get_dummies(dataJobs, columns=['JobType','JobGroup','Site','Status','UserPriority'],\n",
    "                           drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_encoded.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before dropping first column in get_dummies, this returned 4150\n",
    "len(dj_encoded[dj_encoded['Status_Failed']==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After dropping first column in get_dummies, this returned 4150\n",
    "len(dj_encoded[dj_encoded['Status_Failed']==False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "dj_encoded.to_pickle('allJobs_featurized.pickle')\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this finally\n",
    "\n",
    "allJobs_encoded = dj_encoded.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dj_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataJobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del allHBLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### End of all jobs table processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join jobs tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Train split each one into 50%\n",
    "# Concat Test dfs and Train dfs,\n",
    "# Now you have 50% from each class in train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> Join 'allJobs_encoded' and 'allHBLI_wide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHBLI_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allJobs_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_samples = pd.merge(allHBLI_wide, allJobs_encoded, on =['JobID'])\n",
    "print(raw_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_counts = pd.value_counts(raw_samples['JobID'].values, sort=False)\n",
    "plt.hist(sample_counts.values, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Pandas dataframe to LSTM 3D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholdd = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sample_counts.values>=thresholdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision: Number of timesteps per sample = 3\n",
    "# Shape of 3D matrix should be (#samples)x(3)x(132)\n",
    "\n",
    "sum(sample_counts.values<thresholdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows of JobIDs with  fewer than 3 timestamps\n",
    "\n",
    "dff = raw_samples['JobID'].value_counts()\n",
    "print(dff.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = dff[dff<thresholdd]\n",
    "\n",
    "\n",
    "dff = dff.reset_index()\n",
    "dff = dff.rename(index=str, columns={\"JobID\": \"count\", \"index\": \"JobID\"})\n",
    "\n",
    "print(dff.sample(5))\n",
    "print(dff.head())\n",
    "\n",
    "remove_list = dff['JobID'].tolist()\n",
    "remove_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = raw_samples[~raw_samples['JobID'].isin(remove_list)]\n",
    "cleaned.shape, raw_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique JobIDs in cleaned\n",
    "totalleftjobslist = cleaned.JobID.unique().tolist()\n",
    "\n",
    "totalleftjobs = len(cleaned.JobID.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "indices = random.sample(range(len(totalleftjobslist)), int(totalleftjobs * .30))\n",
    "takeoutlist = [totalleftjobslist[i] for i in sorted(indices)]\n",
    "\n",
    "len(takeoutlist), totalleftjobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(remove_list))\n",
    "remove_list = remove_list + takeoutlist\n",
    "print(len(remove_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on cleaned, Test on takeoutdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = raw_samples[~raw_samples['JobID'].isin(remove_list)]\n",
    "cleaned.shape, raw_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "takeoutdf = raw_samples[raw_samples['JobID'].isin(takeoutlist)]\n",
    "takeoutdf.shape, raw_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count values in cleaned\n",
    "\n",
    "cleaned_counts = pd.value_counts(cleaned['JobID'].values, sort=False)\n",
    "sum(cleaned_counts.values<thresholdd), sum(cleaned_counts.values>=thresholdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((sample_counts.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = cleaned.columns.tolist()\n",
    "print(len(train_feat)) #125\n",
    "cleaned.Status_Failed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = cleaned.columns.tolist()\n",
    "print(len(train_feat)) #125\n",
    "\n",
    "train_feat.remove('HeartBeatTime')\n",
    "train_feat.remove('JobID')\n",
    "train_feat.remove('Status_Failed')\n",
    "\n",
    "len(train_feat) #122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Column ('Status_Failed' is the label column)\n",
    "label_col = ['Status_Failed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a df and converts it to 3D tensor\n",
    "# Each sample will have k time steps\n",
    "\n",
    "def samples_features(df_input):\n",
    "    \n",
    "    k = thresholdd\n",
    "    input_cols = train_feat\n",
    "    \n",
    "    # takes a df\n",
    "    # Put your inputs into a single list\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['single_input_vector'] = df_input[input_cols].apply(tuple, axis=1).apply(list)\n",
    "    \n",
    "    # Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements\n",
    "    df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])\n",
    "        \n",
    "    # The starting point\n",
    "    df['cumulative_input_vectors'] = df['single_input_vector'].shift(0)\n",
    "    \n",
    "    for i in range(1,k):\n",
    "        df['cumulative_input_vectors'] += df['single_input_vector'].shift(i)\n",
    "          \n",
    "    df.dropna(inplace=True)     # does operation in place & returns None\n",
    "\n",
    "    # Extract your training data\n",
    "    X_ = np.asarray(df.cumulative_input_vectors)\n",
    "    \n",
    "    # Use hstack to and reshape to make the inputs a 3d vector\n",
    "    X = np.vstack(X_).reshape(len(df), k, len(input_cols))\n",
    "    \n",
    "    # Clean up\n",
    "    del df\n",
    "    \n",
    "    return X\n",
    "    # returns 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholdd #Check it matched the title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build X (each job separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.isnull().values.ravel().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.isnull().values.ravel().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['JobID', 'HeartBeatTime', 'AvailableDiskSpace', 'CPUConsumed',\n",
       "       'LoadAverage', 'MemoryUsed', 'RSS', 'Vsize', 'WallClockTime',\n",
       "       'RescheduleCounter',\n",
       "       ...\n",
       "       'Site_LCG.Torino.it', 'Site_LCG.ULAKBIM.tr', 'Site_Multiple',\n",
       "       'Site_SSH.KMI.jp', 'Status_Failed', 'UserPriority_2', 'UserPriority_3',\n",
       "       'UserPriority_5', 'UserPriority_9', 'UserPriority_10'],\n",
       "      dtype='object', length=316)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.to_pickle('cleaned_for_train_samples.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedgrouped = cleaned.groupby('JobID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 18s, sys: 2.14 s, total: 6min 20s\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collecttrain = cleanedgrouped.apply(samples_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in collecttrain.tolist()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10, 313)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collecttrain.tolist()[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 114 ms, sys: 20.3 ms, total: 134 ms\n",
      "Wall time: 133 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in collecttrain:\n",
    "    #len(x)\n",
    "    for i in x:\n",
    "        X.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194746, 10, 313)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This mixes the jobs - so not good\n",
    "# Decision : Orderting of timesteps from top to bottom is t, t-1, t-2\n",
    "# X = samples(cleaned, k=thresholdd, input_cols = train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Column ('Status_Failed' is the label column)\n",
    "label_col = ['Status_Failed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a df and converts it to 3D tensor\n",
    "# Each sample will have k time steps\n",
    "\n",
    "def samples_label(df_input):\n",
    "    \n",
    "    k = thresholdd\n",
    "    input_cols = label_col\n",
    "    \n",
    "    # takes a df\n",
    "    # Put your inputs into a single list\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['single_input_vector'] = df_input[input_cols].apply(tuple, axis=1).apply(list)\n",
    "    \n",
    "    # Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements\n",
    "    df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])\n",
    "        \n",
    "    # The starting point\n",
    "    df['cumulative_input_vectors'] = df['single_input_vector'].shift(0)\n",
    "    \n",
    "    for i in range(1,k):\n",
    "        df['cumulative_input_vectors'] += df['single_input_vector'].shift(i)\n",
    "          \n",
    "    df.dropna(inplace=True)     # does operation in place & returns None\n",
    "\n",
    "    # Extract your training data\n",
    "    X_ = np.asarray(df.cumulative_input_vectors)\n",
    "    \n",
    "    # Use hstack to and reshape to make the inputs a 3d vector\n",
    "    X = np.vstack(X_).reshape(len(df), k, len(input_cols))\n",
    "    \n",
    "    # Clean up\n",
    "    del df\n",
    "    \n",
    "    return X\n",
    "    # returns 3D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Y (each job separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedgrouped = cleaned.groupby('JobID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "collectlabel = cleanedgrouped.apply(samples_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectlabel.tolist()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10, 1)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectlabel.tolist()[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 121 ms, sys: 12.1 ms, total: 133 ms\n",
      "Wall time: 131 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in collectlabel:\n",
    "    #len(x)\n",
    "    for i in x:\n",
    "        Y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194746, 10, 1)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array(Y)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 µs, sys: 0 ns, total: 29 µs\n",
      "Wall time: 28.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This mixes consecutive jobs - so not good\n",
    "# Decision : Orderting of timesteps from top to bottom is t, t-1, t-2\n",
    "#Y = samples(cleaned, k=thresholdd, input_cols = label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194746, 1)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY = np.array([x[0][0] for x in Y]).reshape(len(Y),1)\n",
    "YY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Y is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unison shuffle\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    import numpy as np\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 723 ms, sys: 1 s, total: 1.72 s\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_final, Y_final = unison_shuffled_copies(X,YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((194746, 10, 313), (194746, 1))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.shape, Y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5147])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_final[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del X\n",
    "del Y\n",
    "del YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final, Y_final = unison_shuffled_copies(X_final, Y_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start: Build test set from 'takeoutdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build X_tes, Y_test using each JobID separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202677, 316)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "takeoutdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    107243\n",
       "1     95434\n",
       "Name: Status_Failed, dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat = takeoutdf.columns.tolist()\n",
    "print(len(train_feat)) #125\n",
    "takeoutdf.Status_Failed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat = takeoutdf.columns.tolist()\n",
    "print(len(train_feat)) #125\n",
    "\n",
    "train_feat.remove('HeartBeatTime')\n",
    "train_feat.remove('JobID')\n",
    "train_feat.remove('Status_Failed')\n",
    "\n",
    "len(train_feat) #122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Column ('Status_Failed' is the label column)\n",
    "label_col = ['Status_Failed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202677, 316)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "takeoutdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "takeoutdf.isnull().values.ravel().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "takeoutdf = takeoutdf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "takeoutdf.isnull().values.ravel().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['JobID', 'HeartBeatTime', 'AvailableDiskSpace', 'CPUConsumed',\n",
       "       'LoadAverage', 'MemoryUsed', 'RSS', 'Vsize', 'WallClockTime',\n",
       "       'RescheduleCounter',\n",
       "       ...\n",
       "       'Site_LCG.Torino.it', 'Site_LCG.ULAKBIM.tr', 'Site_Multiple',\n",
       "       'Site_SSH.KMI.jp', 'Status_Failed', 'UserPriority_2', 'UserPriority_3',\n",
       "       'UserPriority_5', 'UserPriority_9', 'UserPriority_10'],\n",
       "      dtype='object', length=316)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "takeoutdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "takeoutdf.to_pickle('takeoutdf_for_test_samples.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "takeoutdfgrouped = takeoutdf.groupby('JobID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 601 ms, total: 2min 38s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collectXtest = takeoutdfgrouped.apply(samples_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in takeoutdfgrouped.tolist()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 10, 313)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectXtest.tolist()[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.9 ms, sys: 4.06 ms, total: 52.9 ms\n",
      "Wall time: 52.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in collectXtest:\n",
    "    #len(x)\n",
    "    for i in x:\n",
    "        X_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84804, 10, 313)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X_test is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectlabel = takeoutdfgrouped.apply(samples_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in collectlabel.tolist()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 10, 1)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectlabel.tolist()[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_t=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.5 ms, sys: 7.31 ms, total: 57.8 ms\n",
      "Wall time: 56.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in collectlabel:\n",
    "    #len(x)\n",
    "    for i in x:\n",
    "        Y_t.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84804, 10, 1)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_t = np.array(Y_t)\n",
    "Y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 13.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This mixes consecutive jobs - so not good\n",
    "# Decision : Orderting of timesteps from top to bottom is t, t-1, t-2\n",
    "#Y = samples(cleaned, k=thresholdd, input_cols = label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84804, 1)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = np.array([x[0][0] for x in Y_t]).reshape(len(Y_t),1)\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Y_test is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84804, 10, 313), (84804, 1))"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((194746, 10, 313), (194746, 1))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.shape, Y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([44386]), 84804)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_test[:]), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End test set from 'takeoutdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = X_final.shape[1]\n",
    "\n",
    "input_dim = X_final.shape[2]\n",
    "\n",
    "# Output dimensions is the shape of a single output vector\n",
    "# In this case it's just 1, but it could be more\n",
    "output_dim = len(Y_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO SPLIT, SINCE TESTING WILL OCCUR ON leftoutdf taken out earlier\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train,X_test,y_train,y_test = train_test_split(X_final,Y_final,test_size=0.33,random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape,y_train.shape, X_test.shape,  y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5190709950396927"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x[0] for x in Y_final])/ len(Y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_final[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 10, 1)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "input_dim, input_length, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim = input_dim, input_length = input_length, output_dim=output_dim):\n",
    "    print ('Creating model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(20, input_shape=(input_length,input_dim),return_sequences=True))\n",
    "    model.add(LSTM(20))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "    print ('Compiling...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Compiling...\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Bookmark here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Train on 97373 samples, validate on 97373 samples\n",
      "Epoch 1/10\n",
      "97373/97373 [==============================] - 16s 161us/step - loss: 0.1794 - binary_accuracy: 0.9325 - val_loss: 0.0502 - val_binary_accuracy: 0.9870\n",
      "Epoch 2/10\n",
      "97373/97373 [==============================] - 16s 159us/step - loss: 0.0663 - binary_accuracy: 0.9822 - val_loss: 0.0503 - val_binary_accuracy: 0.9870\n",
      "Epoch 3/10\n",
      "97373/97373 [==============================] - 16s 160us/step - loss: 0.0584 - binary_accuracy: 0.9845 - val_loss: 0.3399 - val_binary_accuracy: 0.8882\n",
      "Epoch 4/10\n",
      "97373/97373 [==============================] - 16s 160us/step - loss: 0.0561 - binary_accuracy: 0.9849 - val_loss: 0.0369 - val_binary_accuracy: 0.9916\n",
      "Epoch 5/10\n",
      "97373/97373 [==============================] - 15s 158us/step - loss: 0.0446 - binary_accuracy: 0.9888 - val_loss: 0.0401 - val_binary_accuracy: 0.9901\n",
      "Epoch 6/10\n",
      "97373/97373 [==============================] - 15s 158us/step - loss: 0.0481 - binary_accuracy: 0.9872 - val_loss: 0.0346 - val_binary_accuracy: 0.9919\n",
      "Epoch 7/10\n",
      "97373/97373 [==============================] - 16s 161us/step - loss: 0.0406 - binary_accuracy: 0.9901 - val_loss: 0.0371 - val_binary_accuracy: 0.9911\n",
      "Epoch 8/10\n",
      "97373/97373 [==============================] - 16s 164us/step - loss: 0.0547 - binary_accuracy: 0.9850 - val_loss: 0.0376 - val_binary_accuracy: 0.9917\n",
      "Epoch 9/10\n",
      "97373/97373 [==============================] - 16s 165us/step - loss: 0.0389 - binary_accuracy: 0.9904 - val_loss: 0.0333 - val_binary_accuracy: 0.9921\n",
      "Epoch 10/10\n",
      "97373/97373 [==============================] - 16s 165us/step - loss: 0.0405 - binary_accuracy: 0.9899 - val_loss: 0.0345 - val_binary_accuracy: 0.9912\n",
      "CPU times: user 15min 22s, sys: 5min 53s, total: 21min 16s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print ('Fitting model...')\n",
    "\n",
    "history = model.fit(X_final,Y_final,batch_size=500, epochs=10, validation_split = 0.50, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84804/84804 [==============================] - 13s 155us/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04664756885000598, 0.9893872930522145)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when taking 10 timesteps, and keeping separate test and training JobIDs\n",
    "# loss, accuracy is: \n",
    "# on X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9991642 ],\n",
       "       [0.9991648 ],\n",
       "       [0.99916553],\n",
       "       [0.99916613]], dtype=float32)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.Series([x[0] for x in Y_test])\n",
    "\n",
    "# Use np.rint for rounding off \n",
    "y_predicted = pd.Series([ np.rint(j[0]) for j in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9893872930522145"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.equal(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>418</td>\n",
       "      <td>40418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>482</td>\n",
       "      <td>43904</td>\n",
       "      <td>44386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>40482</td>\n",
       "      <td>44322</td>\n",
       "      <td>84804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0.0    1.0    All\n",
       "True                          \n",
       "0          40000    418  40418\n",
       "1            482  43904  44386\n",
       "All        40482  44322  84804"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_true, y_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that there is no intersection between training and test jobs\n",
    "\n",
    "list(set(takeoutdf.JobID.tolist()) & set(cleaned.JobID.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44322.0"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting all 0s ??\n",
    "sum([ (j) for j in y_predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13097"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(takeoutdf.JobID.unique().tolist()), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint: Valid code above this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix below this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save X to disk\n",
    "\n",
    "\n",
    "\n",
    "#### Create Labels\n",
    "\n",
    "\n",
    "\n",
    "##### Make X_train, X_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Create LSTM\n",
    "\n",
    "\n",
    "\n",
    "##### Train LSTM\n",
    "\n",
    "\n",
    "##### Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_generator():\n",
    "    # for a given JOBID, return a list of vectors\n",
    "    # each vector is a feature vector (observation) at a timestamp t\n",
    "    # length of feature vector = n\n",
    "    \n",
    "    new_dict = {}\n",
    "    # jobID --> { timestamp -> [features], timestamp -> [features], timestamp -> [features], ... }\n",
    "    \n",
    "    ####################################\n",
    "    # Single Table Features\n",
    "    ####################################\n",
    "    \n",
    "    #### Iterate throught each table and pick the feature of interest from each table\n",
    "    \n",
    "    #### Continuous\n",
    "    \n",
    "    #### Categorical\n",
    "    \n",
    "    \n",
    "    ####################################\n",
    "    # Multi-table features (JOINs)\n",
    "    ####################################\n",
    "    \n",
    "    #### Continuous\n",
    "    \n",
    "    #### Categorical\n",
    "    \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_2_samples():\n",
    "    # take the list of vectors for a given job, and reshape it to 3D np array\n",
    "    # each sample could be one-time step or >1 time-step observation\n",
    "    # 3D array : s x t x n\n",
    "    # s = # of samples\n",
    "    # t = # of timesteps in each sample\n",
    "    # n = # length of each feature vector\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_stacker():\n",
    "    # takes samples of a set of jobs\n",
    "    # and stacks these samples on top of each other\n",
    "    # np.append(A, B, axis =0)\n",
    "    # returned 3D array is a set of samples that can be fed to an LSTM\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
