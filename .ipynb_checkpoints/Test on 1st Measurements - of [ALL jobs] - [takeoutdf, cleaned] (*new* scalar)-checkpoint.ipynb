{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Declarations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unison shuffle\n",
    "def unison_shuffled_copies(a, b):\n",
    "    import numpy as np\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyParallel(dfGrouped, func):\n",
    "    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n",
    "    return retLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a df and converts it to 3D tensor\n",
    "# Each sample will have k time steps\n",
    "\n",
    "def samples_features_get_first(df_input):\n",
    "    \n",
    "    k = thresholdd\n",
    "    input_cols = train_feat\n",
    "    \n",
    "    # takes a df\n",
    "    # Put your inputs into a single list\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['single_input_vector'] = df_input[input_cols].head(1).apply(tuple, axis=1).apply(list)\n",
    "    \n",
    "    # Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements\n",
    "    df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])\n",
    "        \n",
    "    # The starting point\n",
    "    df['cumulative_input_vectors'] = df['single_input_vector'].shift(0)\n",
    "    \n",
    "    for i in range(1,k):\n",
    "        df['cumulative_input_vectors'] += df['single_input_vector'].shift(i)\n",
    "          \n",
    "    df.dropna(inplace=True)     # does operation in place & returns None\n",
    "\n",
    "    # Extract your training data\n",
    "    X_ = np.asarray(df.cumulative_input_vectors)\n",
    "    \n",
    "    # Use hstack to and reshape to make the inputs a 3d vector\n",
    "    X = np.vstack(X_).reshape(len(df), k, len(input_cols))\n",
    "    \n",
    "    # Clean up\n",
    "    del df\n",
    "    \n",
    "    return X\n",
    "    # returns 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a df and converts it to 3D tensor\n",
    "# Each sample will have k time steps\n",
    "\n",
    "def samples_label_get_first(df_input):\n",
    "    \n",
    "    k = thresholdd\n",
    "    input_cols = label_col\n",
    "    \n",
    "    \n",
    "    # takes a df\n",
    "    # Put your inputs into a single list\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['single_input_vector'] = df_input[input_cols].head(1).apply(tuple, axis=1).apply(list)\n",
    "    \n",
    "    # Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements\n",
    "    df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])\n",
    "        \n",
    "    # The starting point\n",
    "    df['cumulative_input_vectors'] = df['single_input_vector'].shift(0)\n",
    "    \n",
    "    for i in range(1,k):\n",
    "        df['cumulative_input_vectors'] += df['single_input_vector'].shift(i)\n",
    "          \n",
    "    df.dropna(inplace=True)     # does operation in place & returns None\n",
    "\n",
    "    # Extract your training data\n",
    "    X_ = np.asarray(df.cumulative_input_vectors)\n",
    "    \n",
    "    # Use hstack to and reshape to make the inputs a 3d vector\n",
    "    X = np.vstack(X_).reshape(len(df), k, len(input_cols))\n",
    "    \n",
    "    # Clean up\n",
    "    del df\n",
    "    \n",
    "    return X\n",
    "    # returns 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MYSQL database\n",
    "\n",
    "hostname = 'localhost'\n",
    "username = 'a1singh'\n",
    "password = 'sdsc1234'\n",
    "database = 'belle2'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymysql.connections.Connection object at 0x7ff5cf9a0940>\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "conn = pymysql.connect(host=hostname, user=username, passwd=password, db=database)\n",
    "print(conn)\n",
    "\n",
    "label_col = ['Status_Failed']\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('minmaxscaler.pickle', 'rb') as handle:\n",
    "    scaler = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = ['Status_Failed']\n",
    "\n",
    "train_feat=['AvailableDiskSpace',\n",
    " 'CPUConsumed',\n",
    " 'LoadAverage',\n",
    " 'MemoryUsed',\n",
    " 'RSS',\n",
    " 'Vsize',\n",
    " 'RescheduleCounter',\n",
    " 'JobType_MCProductionBGx0',\n",
    " 'JobType_Merge',\n",
    " 'Site_ARC.DESY.de',\n",
    " 'Site_ARC.SIGNET.si',\n",
    " 'Site_CLOUD.AWS_Sydney.au',\n",
    " 'Site_CLOUD.AWS_Tokyo.jp',\n",
    " 'Site_CLOUD.AWS_Virginia.us',\n",
    " 'Site_CLOUD.CC1_Krakow.pl',\n",
    " 'Site_DIRAC.BINP.ru',\n",
    " 'Site_DIRAC.Beihang.cn',\n",
    " 'Site_DIRAC.CINVESTAV.mx',\n",
    " 'Site_DIRAC.Hokudai.jp',\n",
    " 'Site_DIRAC.IITG.in',\n",
    " 'Site_DIRAC.MIPT.ru',\n",
    " 'Site_DIRAC.Nagoya.jp',\n",
    " 'Site_DIRAC.Nara-WU.jp',\n",
    " 'Site_DIRAC.Niigata.jp',\n",
    " 'Site_DIRAC.Osaka-CU.jp',\n",
    " 'Site_DIRAC.PNNL.us',\n",
    " 'Site_DIRAC.RCNP.jp',\n",
    " 'Site_DIRAC.TIFR.in',\n",
    " 'Site_DIRAC.TMU.jp',\n",
    " 'Site_DIRAC.Test.jp',\n",
    " 'Site_DIRAC.Tokyo.jp',\n",
    " 'Site_DIRAC.UAS.mx',\n",
    " 'Site_DIRAC.UVic.ca',\n",
    " 'Site_DIRAC.Yamagata.jp',\n",
    " 'Site_DIRAC.Yonsei.kr',\n",
    " 'Site_Group.KEK2.jp',\n",
    " 'Site_LCG.CESNET.cz',\n",
    " 'Site_LCG.CNAF.it',\n",
    " 'Site_LCG.CYFRONET.pl',\n",
    " 'Site_LCG.Cosenza.it',\n",
    " 'Site_LCG.DESY.de',\n",
    " 'Site_LCG.Frascati.it',\n",
    " 'Site_LCG.HEPHY.at',\n",
    " 'Site_LCG.KEK.jp',\n",
    " 'Site_LCG.KEK2.jp',\n",
    " 'Site_LCG.KISTI.kr',\n",
    " 'Site_LCG.KIT.de',\n",
    " 'Site_LCG.KMI.jp',\n",
    " 'Site_LCG.Legnaro.it',\n",
    " 'Site_LCG.MPPMU.de',\n",
    " 'Site_LCG.McGill.ca',\n",
    " 'Site_LCG.Melbourne.au',\n",
    " 'Site_LCG.NCHC.tw',\n",
    " 'Site_LCG.NTU.tw',\n",
    " 'Site_LCG.Napoli.it',\n",
    " 'Site_LCG.Pisa.it',\n",
    " 'Site_LCG.Torino.it',\n",
    " 'Site_LCG.ULAKBIM.tr',\n",
    " 'Site_Multiple',\n",
    " 'Site_SSH.KMI.jp',\n",
    " 'UserPriority_2',\n",
    " 'UserPriority_3',\n",
    " 'UserPriority_5',\n",
    " 'UserPriority_9',\n",
    " 'UserPriority_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholdd  = 1\n",
    "\n",
    "def get_predict(listofjobids):\n",
    "    query       = 'SELECT * FROM HeartBeatLoggingInfo WHERE JobID IN (' + ','.join((str(x) for x in listofjobids)) + ')'\n",
    "    allHBLI    = pd.read_sql_query(query,con=conn)\n",
    "    print(\"HBLI table \", allHBLI.shape)\n",
    "\n",
    "    query        = 'SELECT * FROM Jobs WHERE JobID IN (' + ','.join((str(x) for x in listofjobids)) + ')'\n",
    "    dataJobs    = pd.read_sql_query(query,con=conn)\n",
    "    print(\"Jobs table read : \", dataJobs.shape)\n",
    "\n",
    "    # # Process HBLI table: allHBLI\n",
    "    allHBLI.Value = pd.to_numeric(allHBLI['Value'], errors='raise')\n",
    "    allHBLI.HeartBeatTime = pd.to_datetime(allHBLI['HeartBeatTime'], errors='raise')\n",
    "    allHBLI_wide=allHBLI.pivot_table(index=['JobID','HeartBeatTime'], columns='Name', values='Value')\n",
    "    allHBLI_wide = allHBLI_wide.dropna()\n",
    "\n",
    "    allHBLI_wide[['AvailableDiskSpace', 'CPUConsumed', 'LoadAverage', \n",
    "                 'MemoryUsed', 'RSS','Vsize', 'WallClockTime']] = scaler.transform(allHBLI_wide[['AvailableDiskSpace', \n",
    "                                                                            'CPUConsumed', 'LoadAverage', 'MemoryUsed', \n",
    "                                                                            'RSS','Vsize', 'WallClockTime']])\n",
    "    allHBLI_wide = allHBLI_wide.reset_index()\n",
    "    # preserve these columns\n",
    "    dataJobs_columnstokeep = ['JobID', 'JobType', 'Site', 'Status', 'UserPriority','RescheduleCounter']\n",
    "\n",
    "    # Make categorical variables:\n",
    "    dj_encoded = pd.get_dummies(dataJobs, columns=['JobType','Site','Status','UserPriority'],\n",
    "                               drop_first=True)\n",
    "\n",
    "    allJobs_encoded = dj_encoded\n",
    "    \n",
    "    raw_samples = pd.merge(allHBLI_wide, allJobs_encoded, on =['JobID'])\n",
    "    \n",
    "    klist=raw_samples.columns.tolist()\n",
    "    \n",
    "    # Fill for the sites:\n",
    "    for t in train_feat:\n",
    "        if t not in klist:\n",
    "            raw_samples[t]=0\n",
    "    \n",
    "    #display(raw_samples)\n",
    "    \n",
    "    \n",
    "    print(\"cleaned = raw_samples: \", raw_samples.shape)\n",
    "\n",
    "    cleaned = raw_samples.dropna()\n",
    "    print(\"cleaned after dropna: \", cleaned.shape)\n",
    "    cleanedgrouped = cleaned.groupby('JobID')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    collecttrain = applyParallel(cleanedgrouped, samples_features_get_first)\n",
    "\n",
    "\n",
    "    # define list of training features outside\n",
    "    X=[]\n",
    "    for x in collecttrain:\n",
    "        for i in x:\n",
    "            X.append(i)\n",
    "    X_test = np.array(X)\n",
    "    collectlabel = applyParallel(cleanedgrouped, samples_label_get_first)\n",
    "    \n",
    "    Y=[]\n",
    "    for x in collectlabel:\n",
    "        for i in x:\n",
    "            Y.append(i)\n",
    "    Y = np.array(Y)\n",
    "    Y_test = np.array([x[0][0] for x in Y]).reshape(len(Y),1)\n",
    "    \n",
    "    check = 0\n",
    "    \n",
    "    print(\"check, cleaned.shape \",(check, cleaned.shape))\n",
    "    j_checks = cleaned.JobID.sample(20)\n",
    "\n",
    "    for k in j_checks:\n",
    "        \n",
    "        v = cleanedgrouped.get_group(k)\n",
    "        \n",
    "        if(True):\n",
    "            #print(v.columns.tolist())\n",
    "            A = np.array(v)\n",
    "            #print(train_feat)\n",
    "            B = samples_features_get_first(v)\n",
    "            check += 1\n",
    "            \n",
    "            if(len(A) == len(B)):\n",
    "                print(k, 'Lengths match')\n",
    "            \n",
    "            # Tests\n",
    "            for x,y in zip(A,B):\n",
    "                #print(x)\n",
    "                #print(y)\n",
    "                \n",
    "                if not (np.all([ m==n for (m,n) in zip(x[2:8] , y[0][0:6]) ])):\n",
    "                    print(k, 'NOK')\n",
    "                    \n",
    "        if(check == len(j_checks)):\n",
    "            break\n",
    "\n",
    "\n",
    "    print(\"X_tes, Y_test : \", X_test.shape, Y_test.shape)\n",
    "\n",
    "    from keras.models import load_model\n",
    "    model1 = load_model('large_model1_tsteps1_features65_partII.pickle')\n",
    "    loss, accuracy = model1.evaluate(X_test, Y_test)\n",
    "    print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HBLI table  (786086, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (112298, 96)\n",
      "cleaned after dropna:  (2877, 96)\n",
      "check, cleaned.shape  (0, (2877, 96))\n",
      "X_tes, Y_test :  (278, 1, 65) (278, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/a1singh/anaconda3/envs/keras/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py:539: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)\n",
      "/data/home/a1singh/anaconda3/envs/keras/lib/python3.5/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/278 [==============================] - 0s 2ms/step\n",
      "12.845661870009613 0.19424460442374936\n",
      "HBLI table  (641893, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (91699, 96)\n",
      "cleaned after dropna:  (1730, 96)\n",
      "check, cleaned.shape  (0, (1730, 96))\n",
      "X_tes, Y_test :  (192, 1, 65) (192, 1)\n",
      "192/192 [==============================] - 0s 3ms/step\n",
      "15.693284034729004 0.015625\n",
      "HBLI table  (431662, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (61666, 96)\n",
      "cleaned after dropna:  (961, 96)\n",
      "check, cleaned.shape  (0, (961, 96))\n",
      "X_tes, Y_test :  (176, 1, 65) (176, 1)\n",
      "176/176 [==============================] - 1s 4ms/step\n",
      "15.670638691295277 0.017045454545454544\n",
      "HBLI table  (366513, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (52359, 96)\n",
      "cleaned after dropna:  (403, 96)\n",
      "check, cleaned.shape  (0, (403, 96))\n",
      "32247830 Lengths match\n",
      "X_tes, Y_test :  (62, 1, 65) (62, 1)\n",
      "62/62 [==============================] - 1s 10ms/step\n",
      "15.68524855952109 0.016129032258064516\n",
      "HBLI table  (327495, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (46785, 96)\n",
      "cleaned after dropna:  (211, 96)\n",
      "check, cleaned.shape  (0, (211, 96))\n",
      "32269401 Lengths match\n",
      "32264824 Lengths match\n",
      "32263601 Lengths match\n",
      "32269854 Lengths match\n",
      "32269431 Lengths match\n",
      "X_tes, Y_test :  (43, 1, 65) (43, 1)\n",
      "43/43 [==============================] - 1s 17ms/step\n",
      "15.571630699689997 0.023255814646565637\n",
      "HBLI table  (329357, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (47051, 96)\n",
      "cleaned after dropna:  (162, 96)\n",
      "check, cleaned.shape  (0, (162, 96))\n",
      "32277840 Lengths match\n",
      "32274168 Lengths match\n",
      "32277753 Lengths match\n",
      "32280518 Lengths match\n",
      "32277393 Lengths match\n",
      "32274621 Lengths match\n",
      "32281187 Lengths match\n",
      "32272544 Lengths match\n",
      "32272765 Lengths match\n",
      "32278316 Lengths match\n",
      "X_tes, Y_test :  (77, 1, 65) (77, 1)\n",
      "77/77 [==============================] - 1s 10ms/step\n",
      "15.942383927184267 0.0\n",
      "HBLI table  (362173, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (51739, 96)\n",
      "cleaned after dropna:  (141, 96)\n",
      "check, cleaned.shape  (0, (141, 96))\n",
      "32292511 Lengths match\n",
      "X_tes, Y_test :  (23, 1, 65) (23, 1)\n",
      "23/23 [==============================] - 1s 40ms/step\n",
      "15.942383766174316 0.0\n",
      "HBLI table  (449323, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (64189, 96)\n",
      "cleaned after dropna:  (681, 96)\n",
      "check, cleaned.shape  (0, (681, 96))\n",
      "X_tes, Y_test :  (83, 1, 65) (83, 1)\n",
      "83/83 [==============================] - 1s 12ms/step\n",
      "15.558230089854044 0.024096385631934707\n",
      "HBLI table  (495908, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (70844, 96)\n",
      "cleaned after dropna:  (763, 96)\n",
      "check, cleaned.shape  (0, (763, 96))\n",
      "X_tes, Y_test :  (102, 1, 65) (102, 1)\n",
      "102/102 [==============================] - 1s 11ms/step\n",
      "15.78608600766051 0.00980392156862745\n",
      "HBLI table  (434679, 4)\n",
      "Jobs table read :  (10000, 33)\n",
      "cleaned = raw_samples:  (62097, 96)\n",
      "cleaned after dropna:  (453, 96)\n",
      "check, cleaned.shape  (0, (453, 96))\n",
      "32335571 Lengths match\n",
      "32334482 Lengths match\n",
      "32334621 Lengths match\n",
      "32331789 Lengths match\n",
      "32333472 Lengths match\n",
      "32332992 Lengths match\n",
      "X_tes, Y_test :  (116, 1, 65) (116, 1)\n",
      "116/116 [==============================] - 1s 9ms/step\n",
      "15.667515261419888 0.017241379438803113\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import turicreate as tc\n",
    "\n",
    "all_jobids_jobs_table = tc.load_sframe('all_jobids_jobs_table')\n",
    "all_jobids_jobs_table = list(all_jobids_jobs_table['JobID'])\n",
    "\n",
    "for listofjobids in batch(all_jobids_jobs_table[:100000], 10000):\n",
    "    #print([j for j in listofjobids])\n",
    "    get_predict(listofjobids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
